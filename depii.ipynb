{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9678312,"sourceType":"datasetVersion","datasetId":5915391}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:18.943493Z","iopub.execute_input":"2024-10-21T01:54:18.943903Z","iopub.status.idle":"2024-10-21T01:54:18.950016Z","shell.execute_reply.started":"2024-10-21T01:54:18.943864Z","shell.execute_reply":"2024-10-21T01:54:18.949116Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis/twitter_cleaned_data.csv\")  # Replace with your actual data path\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:20.416943Z","iopub.execute_input":"2024-10-21T01:54:20.417321Z","iopub.status.idle":"2024-10-21T01:54:20.655659Z","shell.execute_reply.started":"2024-10-21T01:54:20.417285Z","shell.execute_reply":"2024-10-21T01:54:20.654866Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:29.996691Z","iopub.execute_input":"2024-10-21T01:54:29.997089Z","iopub.status.idle":"2024-10-21T01:54:30.031775Z","shell.execute_reply.started":"2024-10-21T01:54:29.997044Z","shell.execute_reply":"2024-10-21T01:54:30.030834Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data = data.rename(columns={\"category\": \"label\"})\ndata['label'] = data['label'].astype(int)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:30.975253Z","iopub.execute_input":"2024-10-21T01:54:30.976021Z","iopub.status.idle":"2024-10-21T01:54:30.987115Z","shell.execute_reply.started":"2024-10-21T01:54:30.975981Z","shell.execute_reply":"2024-10-21T01:54:30.986136Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"data['label'] = data['label'].replace({-1: 0, 0: 1, 1: 2})\n\nprint(\"Unique labels in dataset:\", data['label'].unique())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:32.184401Z","iopub.execute_input":"2024-10-21T01:54:32.185051Z","iopub.status.idle":"2024-10-21T01:54:32.199633Z","shell.execute_reply.started":"2024-10-21T01:54:32.185009Z","shell.execute_reply":"2024-10-21T01:54:32.198640Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Unique labels in dataset: [0 1 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"data = data.sample(n=30000, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:32.962128Z","iopub.execute_input":"2024-10-21T01:54:32.962783Z","iopub.status.idle":"2024-10-21T01:54:32.983446Z","shell.execute_reply.started":"2024-10-21T01:54:32.962745Z","shell.execute_reply":"2024-10-21T01:54:32.982415Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['cleaned_data'], padding=\"max_length\", truncation=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:40.869115Z","iopub.execute_input":"2024-10-21T01:54:40.869980Z","iopub.status.idle":"2024-10-21T01:54:41.039732Z","shell.execute_reply.started":"2024-10-21T01:54:40.869939Z","shell.execute_reply":"2024-10-21T01:54:41.038777Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\nprint(\"Training dataset format:\", train_dataset)\nprint(\"Testing dataset format:\", test_dataset)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:54:49.186236Z","iopub.execute_input":"2024-10-21T01:54:49.187119Z","iopub.status.idle":"2024-10-21T01:55:00.600896Z","shell.execute_reply.started":"2024-10-21T01:54:49.187079Z","shell.execute_reply":"2024-10-21T01:55:00.599938Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c862e92166af4147a9a744128326a8ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d2873d48b094f1086bc6e2e7cef1798"}},"metadata":{}},{"name":"stdout","text":"Training dataset format: Dataset({\n    features: ['label', 'cleaned_data', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 24000\n})\nTesting dataset format: Dataset({\n    features: ['label', 'cleaned_data', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 6000\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"num_labels = len(train_df['label'].unique())\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",     # evaluation strategy to adopt during training\n    run_name='sentiment_analysis_run'  # Specify a unique run name\n)\n\n# import os\n# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:55:07.838549Z","iopub.execute_input":"2024-10-21T01:55:07.838984Z","iopub.status.idle":"2024-10-21T01:55:08.145486Z","shell.execute_reply.started":"2024-10-21T01:55:07.838947Z","shell.execute_reply":"2024-10-21T01:55:08.144630Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n)\n\ntrainer.train()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T01:55:10.537028Z","iopub.execute_input":"2024-10-21T01:55:10.537876Z","iopub.status.idle":"2024-10-21T02:54:56.083213Z","shell.execute_reply.started":"2024-10-21T01:55:10.537838Z","shell.execute_reply":"2024-10-21T02:54:56.082355Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2250/2250 59:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.568000</td>\n      <td>0.482739</td>\n      <td>0.832333</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.457700</td>\n      <td>0.401690</td>\n      <td>0.862667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.261200</td>\n      <td>0.432241</td>\n      <td>0.861000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2250, training_loss=0.4616586784786648, metrics={'train_runtime': 3584.719, 'train_samples_per_second': 20.085, 'train_steps_per_second': 0.628, 'total_flos': 1.8944166076416e+16, 'train_loss': 0.4616586784786648, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T02:56:25.463522Z","iopub.execute_input":"2024-10-21T02:56:25.463954Z","iopub.status.idle":"2024-10-21T02:57:53.022934Z","shell.execute_reply.started":"2024-10-21T02:56:25.463916Z","shell.execute_reply":"2024-10-21T02:57:53.021834Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [47/47 01:25]\n    </div>\n    "},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.4322410523891449,\n 'eval_accuracy': 0.861,\n 'eval_runtime': 87.5455,\n 'eval_samples_per_second': 68.536,\n 'eval_steps_per_second': 0.537,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\n\nwith open('sentiment_model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T02:58:18.690390Z","iopub.execute_input":"2024-10-21T02:58:18.691325Z","iopub.status.idle":"2024-10-21T02:58:19.702119Z","shell.execute_reply.started":"2024-10-21T02:58:18.691280Z","shell.execute_reply":"2024-10-21T02:58:19.701122Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"./checkpoint\")  # Save the checkpoint in a directory\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T02:58:48.131491Z","iopub.execute_input":"2024-10-21T02:58:48.131900Z","iopub.status.idle":"2024-10-21T02:58:49.089148Z","shell.execute_reply.started":"2024-10-21T02:58:48.131863Z","shell.execute_reply":"2024-10-21T02:58:49.088361Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model.from_pretrained(\"./checkpoint\"),  # Load from the saved checkpoint\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T02:58:59.093278Z","iopub.execute_input":"2024-10-21T02:58:59.093666Z","iopub.status.idle":"2024-10-21T02:58:59.320997Z","shell.execute_reply.started":"2024-10-21T02:58:59.093628Z","shell.execute_reply":"2024-10-21T02:58:59.320090Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}